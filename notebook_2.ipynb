{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I43NxonzOSDg"
      },
      "source": [
        "# Notebook 2 - SQL avec vraies bases de donn√©es\n",
        "## Analyse e-commerce avec PostgreSQL en ligne\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JItQV6o4Ojrm"
      },
      "source": [
        "\n",
        "### üéØ Objectifs p√©dagogiques\n",
        "- Connecter Python √† une vraie base de donn√©es PostgreSQL\n",
        "- √âcrire des requ√™tes SQL complexes sur des donn√©es r√©elles\n",
        "- Impl√©menter des analyses RFM avec SQL\n",
        "- Int√©grer SQL et pandas pour des analyses avanc√©es\n",
        "- G√©rer les connexions et la s√©curit√©\n",
        "\n",
        "### üõçÔ∏è Contexte du projet\n",
        "Vous analysez les donn√©es d'un vrai dataset e-commerce (Brazilian E-Commerce Public Dataset) h√©berg√© sur une base PostgreSQL.\n",
        "\n",
        "Objectif : cr√©er une segmentation client√®le pour optimiser les campagnes marketing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K79TBMVvOuoj"
      },
      "source": [
        "## Partie 1 : Connexion √† la base de donn√©es r√©elle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq7n18iwPBPe"
      },
      "source": [
        "### üîß Installation et configuration\n",
        "\n",
        "\n",
        "# Installation des d√©pendances\n",
        "\n",
        "\n",
        "```\n",
        "pip install psycopg2-binary sqlalchemy pandas python-dotenv\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_NuY2FHuOhu3"
      },
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine, text\n",
        "from sqlalchemy.exc import SQLAlchemyError\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEbORVz5PXMa"
      },
      "source": [
        "### üåê Base de donn√©es PostgreSQL gratuite (ElephantSQL)\n",
        "\n",
        "**Option 1 : ElephantSQL (20MB gratuit)**\n",
        "1. Cr√©ez un compte sur [elephantsql.com](https://www.elephantsql.com/)\n",
        "2. Cr√©ez une instance \"Tiny Turtle\" (gratuite)\n",
        "3. R√©cup√©rez vos credentials\n",
        "\n",
        "**Option 2 : Supabase (500MB gratuit)**\n",
        "1. Cr√©ez un compte sur [supabase.com](https://supabase.com/)\n",
        "2. Cr√©ez un nouveau projet\n",
        "3. R√©cup√©rez l'URL de connexion PostgreSQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "HOST= os.getenv(\"DB_HOST\")\n",
        "DATA_BASE= os.getenv(\"DATA_BASE\")\n",
        "USER= os.getenv(\"DB_USER\")\n",
        "PASSWORD=os.getenv(\"DB_PASSWORD\")\n",
        "PORT= os.getenv(\"DB_PORT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ytLvCF3fQxRJ"
      },
      "outputs": [],
      "source": [
        "# Configuration de connexion (√† adapter selon votre provider)\n",
        "\n",
        "DATABASE_CONFIG = {\n",
        "    'host': HOST,  # Ou votre host Supabase\n",
        "    'database': DATA_BASE,\n",
        "    'user': USER,\n",
        "    'password': PASSWORD,\n",
        "    'port': PORT\n",
        "}\n",
        "# Cr√©ation de l'engine SQLAlchemy\n",
        "engine = create_engine(\n",
        "    f\"postgresql://{DATABASE_CONFIG['user']}:{DATABASE_CONFIG['password']}@\"\n",
        "    f\"{DATABASE_CONFIG['host']}:{DATABASE_CONFIG['port']}/{DATABASE_CONFIG['database']}\"\n",
        ")\n",
        "\n",
        "# Test de connexion\n",
        "def test_connection():\n",
        "    \"\"\"\n",
        "    Testez votre connexion √† la base\n",
        "\n",
        "    √âtapes :\n",
        "    1. Utilisez pd.read_sql() pour ex√©cuter \"SELECT version()\"\n",
        "    2. Affichez la version PostgreSQL\n",
        "    3. G√©rez les erreurs de connexion\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_sql = pd.read_sql(\"SELECT version();\", engine)\n",
        "        print(df_sql)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur de connexion : {e}\")\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             version\n",
            "0  PostgreSQL 17.4 on aarch64-unknown-linux-gnu, ...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXfOgAxGQ3b5"
      },
      "source": [
        "\n",
        "## Partie 2 : Import du dataset e-commerce\n",
        "\n",
        "### üìä Dataset Brazilian E-Commerce\n",
        "Nous utilisons le c√©l√®bre dataset Olist (100k commandes r√©elles).\n",
        "\n",
        "**Tables √† cr√©er :**\n",
        "1. **customers** : customer_id, customer_city, customer_state\n",
        "2. **orders** : order_id, customer_id, order_status, order_date, order_delivered_date\n",
        "3. **order_items** : order_id, product_id, seller_id, price, freight_value\n",
        "4. **products** : product_id, product_category, product_weight_g\n",
        "5. **sellers** : seller_id, seller_city, seller_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GqGIXooNSTjp"
      },
      "outputs": [],
      "source": [
        "### üóÉÔ∏è Cr√©ation des tables SQL\n",
        "def create_tables():\n",
        "    \"\"\"\n",
        "    Cr√©ez les tables dans PostgreSQL\n",
        "\n",
        "    Tips :\n",
        "    - Utilisez des SERIAL pour les IDs auto-increment\n",
        "    - Ajoutez des index sur les cl√©s √©trang√®res\n",
        "    - Incluez des contraintes de validation\n",
        "    \"\"\"\n",
        "\n",
        "    create_customers = \"\"\"    \n",
        "    CREATE TABLE IF NOT EXISTS customers (\n",
        "        customer_id VARCHAR(50) PRIMARY KEY,\n",
        "        customer_city VARCHAR(100) NOT NULL,\n",
        "        customer_state CHAR(2) NOT NULL\n",
        "    );\n",
        "    \"\"\"\n",
        "    create_orders = \"\"\" \n",
        "    CREATE TABLE IF NOT EXISTS orders (\n",
        "        order_id VARCHAR(50) PRIMARY KEY,\n",
        "        customer_id VARCHAR(50) NOT NULL,\n",
        "        order_status VARCHAR(20) CHECK (order_status IN ('delivered', 'shipped', 'processing', 'canceled', 'invoiced', 'unavailable', 'created', 'approved')),\n",
        "        order_date TIMESTAMP NOT NULL,\n",
        "        order_delivered_date TIMESTAMP,\n",
        "        FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n",
        "    );\n",
        "    \"\"\"\n",
        "    create_order_itmes = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS order_items (\n",
        "        order_id VARCHAR(50) NOT NULL,\n",
        "        order_item_id INTEGER NOT NULL,\n",
        "        product_id VARCHAR(50) NOT NULL,\n",
        "        seller_id VARCHAR(50) NOT NULL,\n",
        "        price NUMERIC(10, 2),\n",
        "        freight_value NUMERIC(10, 2),\n",
        "        PRIMARY KEY (order_id, order_item_id),\n",
        "        FOREIGN KEY (order_id) REFERENCES orders(order_id),\n",
        "        FOREIGN KEY (product_id) REFERENCES products(product_id),\n",
        "        FOREIGN KEY (seller_id) REFERENCES sellers(seller_id)\n",
        "    );\n",
        "    \"\"\"\n",
        "    create_product = \"\"\" \n",
        "    CREATE TABLE IF NOT EXISTS products (\n",
        "        product_id VARCHAR(50) PRIMARY KEY,\n",
        "        product_category VARCHAR(100),\n",
        "        product_weight_g INT CHECK (product_weight_g >= 0)\n",
        "    );\n",
        "    \"\"\"\n",
        "    create_sellers = \"\"\" \n",
        "    CREATE TABLE IF NOT EXISTS sellers (\n",
        "        seller_id VARCHAR(50) PRIMARY KEY,\n",
        "        seller_city VARCHAR(100) NOT NULL,\n",
        "        seller_state CHAR(2) NOT NULL\n",
        "    );\n",
        "    \"\"\"\n",
        "    # Compl√©tez pour les autres tables\n",
        "    # N'oubliez pas les contraintes de cl√©s √©trang√®res !\n",
        "\n",
        "    with engine.connect() as conn:\n",
        "        conn.execute(text(create_customers))     # 1\n",
        "        conn.execute(text(create_product))       # 2\n",
        "        conn.execute(text(create_sellers))       # 3\n",
        "        conn.execute(text(create_orders))        # 4\n",
        "        conn.execute(text(create_order_itmes))   # 5\n",
        "        conn.commit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_tables()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "######################################## Start data cleaning ################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load all data frame to start data cleaning\n",
        "df_customer= pd.read_csv(\"archive/olist_customers_dataset.csv\")\n",
        "df_product= pd.read_csv(\"archive/olist_products_dataset.csv\")\n",
        "df_sellers= pd.read_csv(\"archive/olist_sellers_dataset.csv\")\n",
        "df_order= pd.read_csv(\"archive/olist_orders_dataset.csv\")\n",
        "df_order_items= pd.read_csv(\"archive/olist_order_items_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Drops specified columns from the DataFrame\n",
        "def drop_columns(df, columns_to_drop):\n",
        "    \"\"\"\n",
        "    Drops specified columns from the DataFrame.\n",
        "    \n",
        "    Parameters:\n",
        "    - df: pandas DataFrame\n",
        "    - columns_to_drop: list of column names to drop\n",
        "    \n",
        "    Returns:\n",
        "    - DataFrame with specified columns removed\n",
        "    \"\"\"\n",
        "    return df.drop(columns=columns_to_drop, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# creat a list of columns to drop  \n",
        "cols_to_drop = ['customer_unique_id','customer_zip_code_prefix', 'product_name_lenght','product_description_lenght', 'product_photos_qty',\n",
        "            'product_length_cm', 'product_height_cm', 'product_width_cm', 'seller_zip_code_prefix', 'order_approved_at', 'order_delivered_carrier_date',\n",
        "            'order_estimated_delivery_date', 'shipping_limit_date']\n",
        "# run the function on each dataframe \n",
        "df_customer = drop_columns(df_customer, cols_to_drop)\n",
        "df_product = drop_columns(df_product, cols_to_drop)\n",
        "df_sellers = drop_columns(df_sellers, cols_to_drop)\n",
        "df_order = drop_columns(df_order, cols_to_drop)\n",
        "df_order_items = drop_columns(df_order_items, cols_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rename specified columns \n",
        "def rename_columns(df, columns_mapping):\n",
        "    \"\"\"\n",
        "    Renames columns in the DataFrame based on a dictionary mapping.\n",
        "    \n",
        "    Parameters:\n",
        "    - df: pandas DataFrame\n",
        "    - columns_mapping: dict of {old_name: new_name}\n",
        "    \n",
        "    Returns:\n",
        "    - DataFrame with renamed columns\n",
        "    \"\"\"\n",
        "    return df.rename(columns=columns_mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mapping list of columns to rename\n",
        "cols_to_rename = {\n",
        "    \"product_category_name\" : \"product_category\",\n",
        "    \"order_purchase_timestamp\" : \"order_date\",\n",
        "    \"order_delivered_customer_date\" : \"order_delivered_date\"\n",
        "}\n",
        "# Run the function on each data frame\n",
        "df_customer = rename_columns(df_customer, cols_to_rename)\n",
        "df_product = rename_columns(df_product, cols_to_rename)\n",
        "df_sellers = rename_columns(df_sellers, cols_to_rename)\n",
        "df_order = rename_columns(df_order, cols_to_rename)\n",
        "df_order_items = rename_columns(df_order_items, cols_to_rename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Normalizes inconsistent values in the 'order_status' column\n",
        "# def normalize_order_status(df):\n",
        "#     \"\"\"\n",
        "#     Normalizes inconsistent values in the 'order_status' column.\n",
        "#     \"\"\"\n",
        "#     df['order_status'] = df['order_status'].replace('canceled', 'cancelled')\n",
        "#     return df\n",
        "# df_order = normalize_order_status(df_order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_ecommerce_dataframes(dfs):\n",
        "    \"\"\"\n",
        "    Clean and synchronize e-commerce DataFrames in pandas before database creation.\n",
        "    \n",
        "    Removes:\n",
        "    - Rows with missing critical fields\n",
        "    - Duplicates\n",
        "    - Rows violating relationships (foreign key logic within pandas)\n",
        "    \n",
        "    Parameters:\n",
        "    - dfs: dict containing the following DataFrames:\n",
        "        'customers', 'orders', 'order_items', 'products', 'sellers'\n",
        "    \n",
        "    Returns:\n",
        "    - cleaned_dfs: dict with cleaned DataFrames\n",
        "    \"\"\"\n",
        "    \n",
        "    # Unpack DataFrames\n",
        "    customers = dfs.get('customers').copy()\n",
        "    orders = dfs.get('orders').copy()\n",
        "    order_items = dfs.get('order_items').copy()\n",
        "    products = dfs.get('products').copy()\n",
        "    sellers = dfs.get('sellers').copy()\n",
        "    \n",
        "    # 1. Clean customers\n",
        "    customers.dropna(subset=['customer_id', 'customer_city', 'customer_state'], inplace=True)\n",
        "    customers.drop_duplicates(subset=['customer_id'], inplace=True)\n",
        "    \n",
        "    # 2. Clean orders\n",
        "    orders.dropna(subset=['order_id', 'customer_id', 'order_status'], inplace=True)\n",
        "    orders.drop_duplicates(subset=['order_id'], inplace=True)\n",
        "    orders = orders[orders['customer_id'].isin(customers['customer_id'])]\n",
        "    \n",
        "    # 3. Clean products\n",
        "    products.dropna(subset=['product_id'], inplace=True)\n",
        "    products.drop_duplicates(subset=['product_id'], inplace=True)\n",
        "    \n",
        "    # 4. Clean sellers\n",
        "    sellers.dropna(subset=['seller_id'], inplace=True)\n",
        "    sellers.drop_duplicates(subset=['seller_id'], inplace=True)\n",
        "    \n",
        "    # 5. Clean order_items\n",
        "    order_items.dropna(subset=['order_id', 'order_item_id', 'product_id', 'seller_id', 'price', 'freight_value'], inplace=True)\n",
        "    order_items.drop_duplicates(inplace=True)\n",
        "    order_items = order_items[\n",
        "        order_items['order_id'].isin(orders['order_id']) &\n",
        "        order_items['product_id'].isin(products['product_id']) &\n",
        "        order_items['seller_id'].isin(sellers['seller_id'])\n",
        "    ]\n",
        "\n",
        "    # Return cleaned DataFrames\n",
        "    return {\n",
        "        'customers': customers,\n",
        "        'orders': orders,\n",
        "        'order_items': order_items,\n",
        "        'products': products,\n",
        "        'sellers': sellers\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                           order_id  order_item_id  \\\n",
            "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
            "1  00018f77f2f0320c557190d7a144bdd3              1   \n",
            "2  000229ec398224ef6ca0657da4fc703e              1   \n",
            "3  00024acbcdf0a6daa1e931b038114c75              1   \n",
            "4  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
            "\n",
            "                         product_id                         seller_id   price  \\\n",
            "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   58.90   \n",
            "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36  239.90   \n",
            "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d  199.00   \n",
            "3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   12.99   \n",
            "4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87  199.90   \n",
            "\n",
            "   freight_value  \n",
            "0          13.29  \n",
            "1          19.93  \n",
            "2          17.87  \n",
            "3          12.79  \n",
            "4          18.14  \n"
          ]
        }
      ],
      "source": [
        "# load all data\n",
        "dfs = {\n",
        "    'customers': df_customer,\n",
        "    'orders': df_order,\n",
        "    'order_items': df_order_items,\n",
        "    'products':df_product,\n",
        "    'sellers': df_sellers\n",
        "}\n",
        "\n",
        "# clean all columns \n",
        "cleaned_dfs = clean_ecommerce_dataframes(dfs)\n",
        "\n",
        "# print an example to check result\n",
        "print(cleaned_dfs['order_items'].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#####################################--Inserts a DataFrame into a PostgreSQL table using SQLAlchemy--#################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to insert the dataframe into data base \n",
        "def insert_df_to_db(df, table_name, engine):\n",
        "    \"\"\"\n",
        "    Inserts a DataFrame into a PostgreSQL table using SQLAlchemy.\n",
        "    \n",
        "    Parameters:\n",
        "    - df: pandas DataFrame to insert\n",
        "    - table_name: str, the name of the target table in the DB\n",
        "    - engine: SQLAlchemy engine object\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
        "        print(f\"Inserted {len(df)} rows into '{table_name}' successfully.\")\n",
        "    except SQLAlchemyError as e:\n",
        "        print(f\"Failed to insert data into '{table_name}': {e}\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserted 99441 rows into 'customers' successfully.\n",
            "Inserted 32951 rows into 'products' successfully.\n",
            "Inserted 3095 rows into 'sellers' successfully.\n",
            "Inserted 99441 rows into 'orders' successfully.\n",
            "Inserted 112650 rows into 'order_items' successfully.\n"
          ]
        }
      ],
      "source": [
        "# Run the function on each dataframe to insert data into the database\n",
        "\n",
        "# insert_df_to_db(df_customer, \"customers\", engine)\n",
        "# insert_df_to_db(df_product, \"products\", engine)\n",
        "# insert_df_to_db(df_sellers, \"sellers\", engine)\n",
        "# insert_df_to_db(df_order, \"orders\", engine)\n",
        "# insert_df_to_db(df_order_items, \"order_items\", engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBQ_BY-QT4dO"
      },
      "source": [
        "## Partie 3 : Requ√™tes SQL avanc√©es\n",
        "\n",
        "\n",
        "### üîç Analyses SQL √† impl√©menter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdl5RNOBUAV2"
      },
      "source": [
        "#### 1. Analyse RFM (R√©cence, Fr√©quence, Montant)\n",
        "```sql\n",
        "-- Votre d√©fi : Calculer les m√©triques RFM pour chaque client\n",
        "WITH customer_metrics AS (\n",
        "    SELECT\n",
        "        c.customer_id,\n",
        "        c.customer_state,\n",
        "        -- R√©cence : jours depuis dernier achat\n",
        "        -- Fr√©quence : nombre de commandes\n",
        "        -- Montant : total d√©pens√©\n",
        "        \n",
        "        -- Compl√©tez cette requ√™te CTE\n",
        "        \n",
        "    FROM customers c\n",
        "    JOIN orders o ON c.customer_id = o.customer_id\n",
        "    JOIN order_items oi ON o.order_id = oi.order_id\n",
        "    WHERE o.order_status = 'delivered'\n",
        "    GROUP BY c.customer_id, c.customer_state\n",
        ")\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_id</th>\n",
              "      <th>recency</th>\n",
              "      <th>frequency</th>\n",
              "      <th>monetary</th>\n",
              "      <th>recency_score</th>\n",
              "      <th>frequency_score</th>\n",
              "      <th>monetary_score</th>\n",
              "      <th>customer_segment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>033fab69968b0d69099d64423831a236</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>26.40</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>Hibernating</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27ae7c8a8fc20ce80d96f01b6f19961b</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>179.00</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Hibernating</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9e83d47684eb1a58b1c31830f5de10ac</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1</td>\n",
              "      <td>70.00</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>Lost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ffa87b4246c4848711afb512bd51f161</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1</td>\n",
              "      <td>209.99</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Hibernating</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1409b2945191b7aff1975ba2ce9918c5</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1</td>\n",
              "      <td>49.90</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>Lost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7930549f156eea2b01b0fc2fdd323063</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1</td>\n",
              "      <td>69.99</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>Lost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8be45a1114ff0e79615f7b8189aec7df</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1</td>\n",
              "      <td>24.00</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>Hibernating</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7c71fa0871e272a25eeccac52af90595</td>\n",
              "      <td>26.0</td>\n",
              "      <td>1</td>\n",
              "      <td>25.97</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>Hibernating</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>d306426abe5fca15e54b645e4462dc7b</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1</td>\n",
              "      <td>144.99</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>Lost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>a35878bee339b45240b5a327d933509b</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1</td>\n",
              "      <td>29.99</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>Lost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>cac4524e1e6714ef3fdc324fc0f86cfb</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1</td>\n",
              "      <td>364.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Hibernating</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>bfc24858928300e9b18e0d96637b8404</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1</td>\n",
              "      <td>77.90</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>Lost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>b6fbaad4eb3a0794111683dca8122610</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1</td>\n",
              "      <td>166.90</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Hibernating</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>c0789eee49fe7d5d93d5e412c14181ce</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1</td>\n",
              "      <td>127.90</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>Lost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>fc041ede47154c40f55455e20c1a1954</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1</td>\n",
              "      <td>118.13</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>Lost</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         customer_id  recency  frequency  monetary  \\\n",
              "0   033fab69968b0d69099d64423831a236      0.0          1     26.40   \n",
              "1   27ae7c8a8fc20ce80d96f01b6f19961b      5.0          1    179.00   \n",
              "2   9e83d47684eb1a58b1c31830f5de10ac     14.0          1     70.00   \n",
              "3   ffa87b4246c4848711afb512bd51f161     19.0          1    209.99   \n",
              "4   1409b2945191b7aff1975ba2ce9918c5     21.0          1     49.90   \n",
              "5   7930549f156eea2b01b0fc2fdd323063     25.0          1     69.99   \n",
              "6   8be45a1114ff0e79615f7b8189aec7df     25.0          1     24.00   \n",
              "7   7c71fa0871e272a25eeccac52af90595     26.0          1     25.97   \n",
              "8   d306426abe5fca15e54b645e4462dc7b     27.0          1    144.99   \n",
              "9   a35878bee339b45240b5a327d933509b     27.0          1     29.99   \n",
              "10  cac4524e1e6714ef3fdc324fc0f86cfb     27.0          1    364.00   \n",
              "11  bfc24858928300e9b18e0d96637b8404     29.0          1     77.90   \n",
              "12  b6fbaad4eb3a0794111683dca8122610     29.0          1    166.90   \n",
              "13  c0789eee49fe7d5d93d5e412c14181ce     29.0          1    127.90   \n",
              "14  fc041ede47154c40f55455e20c1a1954     29.0          1    118.13   \n",
              "\n",
              "    recency_score  frequency_score  monetary_score customer_segment  \n",
              "0               1                2               5      Hibernating  \n",
              "1               1                2               1      Hibernating  \n",
              "2               1                4               3             Lost  \n",
              "3               1                1               1      Hibernating  \n",
              "4               1                3               4             Lost  \n",
              "5               1                4               3             Lost  \n",
              "6               1                2               5      Hibernating  \n",
              "7               1                2               5      Hibernating  \n",
              "8               1                5               2             Lost  \n",
              "9               1                3               5             Lost  \n",
              "10              1                1               1      Hibernating  \n",
              "11              1                4               3             Lost  \n",
              "12              1                2               2      Hibernating  \n",
              "13              1                5               2             Lost  \n",
              "14              1                5               2             Lost  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rfm_query = \"\"\"\n",
        "--Calculate  RFM metrics (Recency, Frequency, Monetary)\n",
        "WITH rfm_base AS (\n",
        "    SELECT\n",
        "        c.customer_id,\n",
        "        DATE_PART('day', DATE '2018-10-17' - MAX(o.order_delivered_date)) AS recency,\n",
        "        COUNT(o.order_id) AS frequency,\n",
        "        SUM(oi.price) AS monetary\n",
        "    FROM customers c\n",
        "    JOIN orders o ON c.customer_id = o.customer_id\n",
        "    JOIN order_items oi ON o.order_id = oi.order_id\n",
        "    WHERE o.order_status = 'delivered'\n",
        "    GROUP BY c.customer_id\n",
        "),\n",
        "\n",
        "--Assign RFM scores from 1 (worst) to 5 (best)\n",
        "scored_rfm AS (\n",
        "    SELECT\n",
        "        customer_id,\n",
        "        recency,\n",
        "        frequency,\n",
        "        monetary,\n",
        "        NTILE(5) OVER (ORDER BY recency ASC) AS recency_score,   -- More recent = higher score\n",
        "        NTILE(5) OVER (ORDER BY frequency DESC) AS frequency_score, -- More orders = higher score\n",
        "        NTILE(5) OVER (ORDER BY monetary DESC) AS monetary_score    -- More spending = higher score\n",
        "    FROM rfm_base\n",
        ")\n",
        "\n",
        "--Assign customer segments based on RFM scores\n",
        "SELECT\n",
        "    *,\n",
        "    CASE\n",
        "        WHEN recency_score >= 4 AND frequency_score >= 4 AND monetary_score >= 4 THEN 'Champions'\n",
        "        WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'\n",
        "        WHEN recency_score >= 4 AND frequency_score BETWEEN 2 AND 3 THEN 'Potential Loyalists'\n",
        "        WHEN recency_score BETWEEN 2 AND 3 AND frequency_score >= 3 THEN 'At Risk'\n",
        "        WHEN recency_score <= 2 AND frequency_score <= 2 THEN 'Hibernating'\n",
        "        WHEN recency_score = 1 THEN 'Lost'\n",
        "        ELSE 'Others'\n",
        "    END AS customer_segment\n",
        "FROM scored_rfm;\n",
        "\"\"\"\n",
        "with engine.connect() as conn:\n",
        "    df_rfm = pd.read_sql(text(rfm_query), conn)\n",
        "\n",
        "df_rfm.head(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWF9rpZSUMp5"
      },
      "outputs": [],
      "source": [
        "#### 2. Analyse g√©ographique des ventes\n",
        "\n",
        "def geographic_sales_analysis():\n",
        "    \"\"\"\n",
        "    Analysez les performances par √©tat/r√©gion\n",
        "\n",
        "    Requ√™tes √† √©crire :\n",
        "    1. Top 10 des √©tats par CA\n",
        "    2. Croissance MoM par r√©gion\n",
        "    3. Taux de conversion par ville\n",
        "    4. Distance moyenne vendeur-acheteur\n",
        "    \"\"\"\n",
        "\n",
        "    query_top_states = \"\"\"\n",
        "    -- Votre requ√™te SQL ici\n",
        "    -- Utilisez des JOINs et GROUP BY\n",
        "    -- Calculez le CA, nombre de commandes, panier moyen\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.read_sql(query_top_states, engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OE-UHLKY8-K"
      },
      "source": [
        "#### 3. Analyse temporelle et saisonnalit√©\n",
        "```sql\n",
        "-- D√©tectez les patterns saisonniers\n",
        "SELECT\n",
        "    EXTRACT(YEAR FROM order_date) as year,\n",
        "    EXTRACT(MONTH FROM order_date) as month,\n",
        "    EXTRACT(DOW FROM order_date) as day_of_week,\n",
        "    COUNT(*) as order_count,\n",
        "    SUM(price + freight_value) as total_revenue,\n",
        "    AVG(price + freight_value) as avg_order_value\n",
        "FROM orders o\n",
        "JOIN order_items oi ON o.order_id = oi.order_id\n",
        "WHERE order_status = 'delivered'\n",
        "GROUP BY ROLLUP(\n",
        "    EXTRACT(YEAR FROM order_date),\n",
        "    EXTRACT(MONTH FROM order_date),\n",
        "    EXTRACT(DOW FROM order_date)\n",
        ")\n",
        "ORDER BY year, month, day_of_week;\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq43e3mfZC8d"
      },
      "source": [
        "## Partie 4 : Analyse pr√©dictive avec SQL\n",
        "\n",
        "### üîÆ Mod√®les simples en SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY5mfxFoaL2K"
      },
      "outputs": [],
      "source": [
        "#### 1. Pr√©diction de churn\n",
        "\n",
        "def churn_prediction_sql():\n",
        "    \"\"\"\n",
        "    Identifiez les clients √† risque de churn\n",
        "\n",
        "    Indicateurs :\n",
        "    - Pas d'achat depuis X jours\n",
        "    - Baisse de fr√©quence d'achat\n",
        "    - Diminution du panier moyen\n",
        "    - Changement de comportement g√©ographique\n",
        "    \"\"\"\n",
        "\n",
        "    churn_query = \"\"\"\n",
        "    WITH customer_activity AS (\n",
        "        -- Calculez les m√©triques d'activit√© r√©cente\n",
        "        -- Comparez avec l'historique du client\n",
        "        -- Scorez le risque de churn\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "        customer_id,\n",
        "        days_since_last_order,\n",
        "        order_frequency_trend,\n",
        "        monetary_trend,\n",
        "        churn_risk_score,\n",
        "        CASE\n",
        "            WHEN churn_risk_score > 0.7 THEN 'High Risk'\n",
        "            WHEN churn_risk_score > 0.4 THEN 'Medium Risk'\n",
        "            ELSE 'Low Risk'\n",
        "        END as churn_segment\n",
        "    FROM customer_activity;\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.read_sql(churn_query, engine)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB2D1PDraVu4"
      },
      "source": [
        "#### 2. Recommandations produits\n",
        "```sql\n",
        "-- Market Basket Analysis simplifi√©\n",
        "WITH product_pairs AS (\n",
        "    SELECT\n",
        "        oi1.product_id as product_a,\n",
        "        oi2.product_id as product_b,\n",
        "        COUNT(*) as co_purchase_count\n",
        "    FROM order_items oi1\n",
        "    JOIN order_items oi2 ON oi1.order_id = oi2.order_id\n",
        "    WHERE oi1.product_id != oi2.product_id\n",
        "    GROUP BY oi1.product_id, oi2.product_id\n",
        "    HAVING COUNT(*) >= 10  -- Seuil minimum\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    product_a,\n",
        "    product_b,\n",
        "    co_purchase_count,\n",
        "    co_purchase_count::float / total_a.count as confidence\n",
        "FROM product_pairs pp\n",
        "JOIN (\n",
        "    SELECT product_id, COUNT(*) as count\n",
        "    FROM order_items\n",
        "    GROUP BY product_id\n",
        ") total_a ON pp.product_a = total_a.product_id\n",
        "ORDER BY confidence DESC;\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbYkj8ItabH-"
      },
      "source": [
        "## Partie 5 : Int√©gration avec les APIs m√©t√©o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4CU6SNEfNXb"
      },
      "source": [
        "### üå§Ô∏è Croisement donn√©es m√©t√©o/ventes\n",
        "```python\n",
        "def weather_sales_correlation():\n",
        "    \"\"\"\n",
        "    Correlez vos donn√©es m√©t√©o du Notebook 1 avec les ventes\n",
        "    \n",
        "    Hypoth√®ses √† tester :\n",
        "    1. Les ventes de certaines cat√©gories augmentent-elles avec la pluie ?\n",
        "    2. Y a-t-il un impact de la temp√©rature sur les achats ?\n",
        "    3. Les livraisons sont-elles impact√©es par la m√©t√©o ?\n",
        "    \"\"\"\n",
        "    \n",
        "    # R√©cup√©rez les donn√©es m√©t√©o historiques pour les villes br√©siliennes\n",
        "    weather_query = \"\"\"\n",
        "    SELECT DISTINCT customer_city, customer_state\n",
        "    FROM customers\n",
        "    WHERE customer_state IN ('SP', 'RJ', 'MG', 'RS', 'SC')\n",
        "    ORDER BY customer_city;\n",
        "    \"\"\"\n",
        "    \n",
        "    cities = pd.read_sql(weather_query, engine)\n",
        "    \n",
        "    # Int√©grez avec l'API m√©t√©o\n",
        "    # Analysez les corr√©lations\n",
        "    \n",
        "    pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHG9k_5PfZXd"
      },
      "source": [
        "### üìä Dashboard g√©o-temporel\n",
        "```python\n",
        "def create_geotemporal_dashboard():\n",
        "    \"\"\"\n",
        "    Cr√©ez un dashboard interactif combinant :\n",
        "    - Carte des ventes par r√©gion\n",
        "    - √âvolution temporelle avec m√©t√©o\n",
        "    - Segments clients g√©olocalis√©s\n",
        "    - Pr√©dictions par zone g√©ographique\n",
        "    \"\"\"\n",
        "    pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsIuD-IVfnxW"
      },
      "source": [
        "---\n",
        "## üèÜ Livrables finaux\n",
        "\n",
        "### üìà Rapport d'analyse complet\n",
        "1. **Segmentation RFM (Recency, Frenquency, Monetary) ** : 5-7 segments avec caract√©ristiques\n",
        "2. **Analyse g√©ographique**  : Performances par r√©gion + recommandations\n",
        "3. **Pr√©dictions churn** : Liste des clients √† risque + actions\n",
        "4. **Recommandations produits** : Top 10 des associations\n",
        "5. **Impact m√©t√©o** : Corr√©lations significatives identifi√©es\n",
        "\n",
        "### üöÄ Pipeline automatis√©\n",
        "```python\n",
        "def automated_analysis_pipeline():\n",
        "    \"\"\"\n",
        "    Pipeline qui :\n",
        "    1. Se connecte √† la DB\n",
        "    2. Ex√©cute toutes les analyses\n",
        "    3. Met √† jour les segments clients\n",
        "    4. G√©n√®re le rapport automatiquement\n",
        "    5. Envoie des alertes si n√©cessaire\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wynvmdtNftwf"
      },
      "source": [
        "## üéì Auto-√©valuation\n",
        "\n",
        "- [ ] **Connexion DB** : PostgreSQL fonctionnelle\n",
        "- [ ] **Requ√™tes complexes** : JOINs, CTEs, fonctions analytiques\n",
        "- [ ] **Gestion des erreurs** : Connexions robustes\n",
        "- [ ] **Performance** : Requ√™tes optimis√©es avec index\n",
        "- [ ] **Int√©gration** : SQL + Python + APIs\n",
        "- [ ] **Insights actionables** : Recommandations business claires\n",
        "\n",
        "### üîó Pr√©paration au Notebook 3\n",
        "Le prochain notebook portera sur NoSQL (MongoDB) avec des donn√©es de r√©seaux sociaux et d'IoT, en temps r√©el.\n",
        "\n",
        "### üí° Bases de donn√©es alternatives\n",
        "- **PlanetScale** : MySQL serverless gratuit\n",
        "- **MongoDB Atlas** : 512MB gratuit\n",
        "- **FaunaDB** : Base multi-mod√®le gratuite\n",
        "- **Hasura Cloud** : GraphQL + PostgreSQL"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
